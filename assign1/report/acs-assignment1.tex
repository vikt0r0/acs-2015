% !TEX TS-program = pdflatex
\documentclass[11pt,a4paper,english]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[obeyspaces, hyphens]{url}
\usepackage[top=4cm, bottom=4cm, left=3cm, right=3cm]{geometry}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{mdwlist}
\usepackage{fancyhdr}
\usepackage{cite}
\usepackage{amsmath}
\usepackage[normalem]{ulem} % ulem enables strikethrough and more, but makes
                            % \emph underline by default :(
\usepackage{babel}
\usepackage{fancyvrb}
\usepackage{verbatimbox}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{minted}
\usepackage{xcolor}
\usepackage{csquotes}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{array}
\usepackage{lmodern} % better font
\usepackage{hyperref} % always load hyper ref in the end
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\newcommand*\justify{%
  \fontdimen2\font=0.4em% interword space
  \fontdimen3\font=0.2em% interword stretch
  \fontdimen4\font=0.1em% interword shrink
  \fontdimen7\font=0.1em% extra space
  \hyphenchar\font=`\-% allowing hyphenation
}

\lstset{
    frame=lrtb,
    captionpos=b,
    belowskip=0pt
}

\captionsetup[listing]{aboveskip=5pt,belowskip=\baselineskip}


%\definecolor{lightgray}{rgb}{0.95,0.95,0.95}
%\renewcommand\listingscaption{Code}

\newcommand{\concat}{\ensuremath{+\!\!\!\!+\!\!}}

\pagestyle{fancy}
\headheight 35pt

\DefineVerbatimEnvironment{code}{Verbatim}{fontsize=\small}
\DefineVerbatimEnvironment{example}{Verbatim}{fontsize=\small}
\newcommand{\ignore}[1]{}

\hyphenation{character-ised}

\rhead{Assignment 1}
\lhead{ACS}
\begin{document}

\thispagestyle{empty} %fjerner sidetal
\hspace{6cm} \vspace{6cm}
\begin{center}
\textbf{\Huge {Advanced Computer Systems}}\\ \vspace{0.5cm}
\Large{Assignment 1}
\end{center}
\vspace{3cm}
\begin{center}
\Large{\textbf{Truls Asheim, Rasmus Wriedt Larsen, Viktor Hansen}}
\end{center}
\vspace{6.0cm}
\thispagestyle{empty}

\newpage

\section{Exercises}
\subsection{Questions 1: Fundamental Abstractions}
\begin{enumerate}
\item{
The top-level abstraction makes use of location addressed memory, i.e. an N-bit memory address mapping a byte of the address space. Each machine $m_i$ in the cluster contains $n_i$ bytes of memory and the memory is directly mapped to address ranges in the single address space in a contiguous manner. This entails that the only valid addresses of the address space are 0 through $\sum_{i=1}^{K} n_{i}$.

The naming scheme consists of issuing READ/WRITE request to a central server, which looks up what intervals of the memory map to which machines. The lookup function is implemented with a table. A name identifying the machine is returned to the client, which in turn issues the request to machine holding the requested data.

The design is scalable in the sense that adding a machine machine, $m_{K+1}$, to the array essentially means adding a table entry with the interval corresponding to the address space range in constant time. In this case the entry $([\sum_{i=1}^{K} n_{i}; \sum_{i=1}^{K+1} n_{i}], m_{K+1})$. This increases the range of valid addresses by $n_{k+1}$.

Client-machine communication is assumed to facilitate positive acknowledgments with retransmissions for requests. In the case a request can not be properly delivered, the central server or machine holding the requested data is assumed to be in an erronous state or unavailable. If the central server is down, it will not be possible}
\end{enumerate}


\subsection{Question 2: Techniques for Performance}

\begin{enumerate}
\item Concurrency enables us to process more than one request at once, which can
  reduce the average latency over serial execution, as all requests do not need
  to wait for a slow IO device to complete (ie disk access). However, this
  assumes the different threads do not spend too much time waiting to acquire
  locks, or even deadlocks.

\item \emph{Batching} is submitting more than one request to a latency heavy
  service, to amortize the cost of the latency and improve
  throughput. \emph{Dallying} is waiting to fill up buffers before batching
  requests, to further improve throughput, but will most likely increase
  latency.

\item Caching the results to frequent requests is a way of fast path
  optimization, as we do not have to perform the whole computation every time.
\end{enumerate}


%\bibliographystyle{plain}
%\bibliography{references}

%\section*{Appendix A}
%\subsection*{\path{src/commented-disassembly.txt}}
%\inputminted{text}{src/commented-disassembly.txt}

%\pagebreak

%\subsection*{\path{src/exploit.c}}
%\inputminted{c}{src/exploit.c}

\end{document}
