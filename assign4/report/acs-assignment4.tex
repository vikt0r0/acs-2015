% !TEX TS-program = pdflatex
\documentclass[11pt,a4paper,english]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[obeyspaces, hyphens]{url}
\usepackage[top=4cm, bottom=4cm, left=3cm, right=3cm]{geometry}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{mdwlist}
\usepackage{fancyhdr}
\usepackage{cite}
\usepackage{amsmath}
\usepackage[normalem]{ulem} % ulem enables strikethrough and more, but makes
                            % \emph underline by default :(
\usepackage{babel}
\usepackage{fancyvrb}
\usepackage{verbatimbox}
\usepackage{amsfonts}
\usepackage{amsthm}
%\usepackage{minted}
\usepackage{xcolor}
\usepackage{csquotes}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{courier}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{array}
\usepackage{lmodern} % better font
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage{paralist}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{tikz}
\usetikzlibrary{calc, arrows, decorations.markings}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{hyperref} % always load hyper ref in the end
\usepackage{cleveref} % except cleveref
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}

\newcommand*\justify{%
  \fontdimen2\font=0.4em% interword space
  \fontdimen3\font=0.2em% interword stretch
  \fontdimen4\font=0.1em% interword shrink
  \fontdimen7\font=0.1em% extra space
  \hyphenchar\font=`\-% allowing hyphenation
}

\lstset{
    frame=lrtb,
    captionpos=b,
    belowskip=0pt
}

\captionsetup[listing]{aboveskip=5pt,belowskip=\baselineskip}

\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO: }#1}}

%\definecolor{lightgray}{rgb}{0.95,0.95,0.95}
%\renewcommand\listingscaption{Code}

\newcommand{\concat}{\ensuremath{+\!\!\!\!+\!\!}}

\pagestyle{fancy}
\headheight 35pt

\DefineVerbatimEnvironment{code}{Verbatim}{fontsize=\small}
\DefineVerbatimEnvironment{example}{Verbatim}{fontsize=\small}
\newcommand{\ignore}[1]{}

\hyphenation{character-ised}

\rhead{Assignment 4}
\lhead{ACS}

\input{plots}

\begin{document}

\thispagestyle{empty} %fjerner sidetal
\hspace{6cm} \vspace{6cm}
\begin{center}
\textbf{\Huge {Advanced Computer Systems}}\\ \vspace{0.5cm}
\Large{Assignment 4}
\end{center}
\vspace{3cm}
\begin{center}
\Large{\textbf{Truls Asheim, Rasmus Wriedt Larsen, Viktor Hansen}}
\end{center}
\vspace{6.0cm}
\thispagestyle{empty}

\newpage

\section*{Exercises}

\subsection*{Question 1: Recovery Concepts}
\begin{enumerate}
\item In a system forcing writes to disk when a transaction commits, it is \emph{not}
  necessary to implement a scheme for redo, as all committed data will be on
  disk. This is based on the assumption that the system does not crash in the
  middle of writing committed data to disk, which is a very crude assumption.

  In a system with no-steal, we do \emph{not} need to implement at scheme for
  redo, as no uncommitted data will be written to disk.

\item Nonvolatile storage (such as disk) can survive a system crash, but can
  have media failures. Data stored in stable storage is assumed to never be
  lost, even though this does usually not consider extreme cases such as atomic
  bombs. Therefore the major difference is the resources/money required to
  implement either.

\item The rules of Write-Ahead Logging are 1) must write log for data updates
  before writing the new data to disk, and 2) must write all log entries for a
  transaction just before committing.

  These two rules ensures durability (results are persistent). Rules 1) ensures
  that we can undo changes that have been written to disk from transactions that
  have not committed, and rule 2) ensures that if the system crashes immediately
  after a commit (and before data is written), we still have all the necessary
  information to redo the operation.
\end{enumerate}

\subsection*{Question 2: ARIES}
\begin{enumerate}
\item The transaction and dirty page table is shown in Table \ref{tbl:dirty} and \ref{tbl:transaction}, respectively.
\begin{table}[!hbt]
\parbox{.45\linewidth}{
\centering
\begin{tabular}{|l|l|}
\hline
pageID  & recLSN  \\ \hline
P2      & 3       \\ \hline
P1      & 4       \\ \hline
P5      & 5       \\ \hline
P3      & 6       \\ \hline
\end{tabular}
\caption{Dirty page table}
\label{tbl:dirty}
}
\hfill
\parbox{.45\linewidth}{
\centering
\begin{tabular}{|l|l|}
\hline
transID & lastLSN \\ \hline
T1      & 4       \\ \hline
T2      & 9       \\
\hline
\end{tabular}
\caption{Transaction table}
\label{tbl:transaction}
}
\end{table}

\item Loser transactions are the transactions present in the transaction table with an associated \texttt{lastLSN} value, those are $\left\{ \texttt{T1}, \texttt{T2} \right\}$. The single winner transaction is \texttt{T3}.

\item The redo-phase starts by re-applying the log entry with the smallest LSN that updates a page, i.e. the entry with LSN 3. The undo phase stops at the update entry with the lowest LSN among all loser transactions, in this case the entry with LSN 3.

\item Entries 8 and 9 as well as 5 and 6 as they update pages P5 and P3, respectively.

\item The records to be undone are all the entries associated with the loser transactions, i.e. $\left\{9, 8, 5, 4, 3\right\}$.

\item Here, a CLR is written to the log for each update record encountered in the undo phase. The resulting log will be the original logfile shown in the assignent prepended to one shown in Listing \ref{lst:log}.
\\
\begin{lstlisting}[caption={Compensation log records for the undo-phase.},label={lst:log},escapeinside={@}{@}]
LSN LAST_LSN TRAN_ID TYPE PAGE_ID  UNDO_NEXT_LSN
--- -------- ------- ---- -------  -------------
 11 NULL     T2      NULL P3       8
 12 NULL     T2      NULL P5       5
 13 NULL     T2      NULL P5       NULL
 14 NULL     T1      NULL P1       3
 15 NULL     T1      NULL P2       NULL
\end{lstlisting}

\end{enumerate}

\section*{Programming Task}
\subsection*{Overview of Implementation}
\subsection*{Discussion on the Performance Measurements}
\begin{enumerate}
\item We initially generate a set of books possible to add to the bookstore,
  which we then use a subset of as the initial books in the bookstore.

  We use \verb|numBooksToAdd| with a value of 5. As 10\% of the $500+100 = 600$
  runs by a worker are ``addBooks interactions'', we will get $60$ such
  interactions per worker, thus resulting in $60 \times 5 = 300$ books attempted
  to be added per worker. Ignoring the effects of random sampling, we would need
  $300 \times \mathtt{numConcurrentWorkloadThreads}$ books in the universe
  initially not added to the bookstore, to make sure every ``addBooks
  interactions'' would expand the bookstore with 5 new books. Note that an
  attempt to add 0 new books will be counted as a successful interaction, and
  will still require to go (briefly) into the \verb|addBooks| method thereby
  congesting the queue for access to the synchronization.

  In our experiments we run a maximum of 1000 threads, and create a
  universe of 1000 books (controlled by \verb|NUM_TOTAL_BOOKS|) and initially
  add 400 books to the bookstore (\verb|NUM_INITIAL_ADD_BOOKS|).

  Book generation is done pseudo-randomly for each book, with ISBNs being
  $1,2,\cdots$. A book becomes an editor pick with a chance of $10\%$, number of
  copies is uniformly randomly picked from the integer interval $[1,20]$. Title
  and author name is randomly generated from the characters \verb|[a-zA-Z]| with
  a length determined by a Gaussian distribution -- but at least length 2. The
  title length has a mean of 10 and a standard deviation of 3, while the author
  name length has a mean of 15 and a standard deviation of 3. These values where
  picked using a rough estimate after look at a bookshelf. Book price is
  irrelevant to this assignment, but is uniformly random in the interval
  $[150,350)$.

  The benchmarks has been performed on a laptop with 1.9GHz Core i7-3517U CPU and
  8GB of RAM running Linux with kernel version 4.1.12. The JVM used is OpenJDK
  1.8.0\_60.

  We perform 100 warmup runs and 500 actual runs (unchanged from the handout
  code). Each benchmark has been run 5 times (when possible, see next question)
  and the presented result is the simple average of these runs.

  The measurements are performed by the client and includes logging of several
  parameters including the total elapsed, number of total runs, number of
  successful runs and number of high-frequency interactions. These parameters
  are logged per-thread and we use them to calculate the latency and throughput
  experienced by clients interacting with the bookstore. The formula for the latency is
  $\frac{1}{N}\sum_{i=0}^N\frac{t_i}{s_i}$ where $N$ is the total number of
  threads, $t_i$ is the total time elapsed and $s_i$ is the number of successful
  interactions. The formula for the aggregated throughput is
  $\sum_{i=0}^N\frac{s_i}{t_i}$.



\item

% \acsplot{local data}{rpc data}{y axis label}{max}

\begin{figure}[H]
  \centering
  \acsplot{\throughputlocal}{\throughputrpc}{Throughput (req/ms)}{50}
  \caption{Development in throughput when running different number of
    client threads, when running the backend in same addressspace (local) or
    over RPC.}
  \label{fig:throughput}
\end{figure}

\autoref{fig:throughput} shows the throughput when increasing the number of
concurrent client threads. Generally we see the expected pattern, the more
threads that are running, the more throughput we get. For the local case, we see
that the throughput grow linearly when increasing the number of clients
exponentially. Thus the throughput could be approximated by a logarithmic
function of the number of concurrent clients.

Throughput when running over RPC was extremely slow compared to running
locally. We got 0.057 requests through per ms when using 1 client thread, and
0.086 requests per ms when using 10 client threads. Using more than this lead to
such slow execution times that it was not feasible to study in detail. We did
complete a run with 100 client threads (which took 20 minutes), which had a
throughput of 0.044.

\begin{figure}[H]
  \centering
  \acsplot{\latencylocal}{\latencyrpc}{Latency (ms)}{120}
  \caption{Development in latency when running different number of
    client threads, when running the backend in same addressspace (local) or
    over RPC.}
  \label{fig:latency}
\end{figure}

\autoref{fig:latency} show the latency when increasing the number of concurrent
client threads. Again we have not been able to run the RPC in the configurations
we wanted. We see that as the number of clients starts to rise, the latency is
increased. This also seems natural; when there are many threads executing on the
server side, a single request needs to wait longer before being processed, as it
has to fight for access to resources.

In the configuration with 100 client threads (that took 20 minutes), we saw an average
latency of 2.2 seconds.

\item The reliability of the metrics depends on whether the mix of requests modeled by the workloads are representative for the actual distribution of requests. Furthermore, a higher number of runs would entail a higher degree of certainty for the measured metrics. Several other performance-related metrics might be interesting to measure for the bookstore. Some could be implemented with relative ease by modifying the solution to the workload code. For instance, it might be insightful to stress test the service for a duration corresponding to the expected uptime (between restarting the service, e.g. when patching it, etc.). This requires that the request rate and mix should representative for expected usage of the service. Furthermore, some robustness aspects could be interesting to measure. For instance if the service has memory leaks at peak loads. A scalability measure could be approximated by adding more cores and looking for correlations between number of cores and a decrease/increase in latency/throughput.
\end{enumerate}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
